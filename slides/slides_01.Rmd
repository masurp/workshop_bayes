---
title: "Introduction to Bayesian Statistics"
subtitle: ""
author: "Philipp K. Masur"
institute: ""
date: "Mainz | 10.-11. October 2022"
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: false
      countIncrementalSlides: false
      ratio: 16:9
      titleSlideClass: [left, middle]
---

```{r child = 'theme.rmd'}
```


class: inverse, center, middle

# Welcome

Why are you interested in Bayesian Statistics?

Any prior experience?


---

# Expectations and overview

.pull-left[

### What we will cover

- Bayes Theorem and Bayesian Inference

- How to run (generalized) linear models using 'brms'

- How to specify priors and interpret results

- How to draw probabilistic inferences from results

]

.pull-left[

### What we will NOT cover

- Introduction to R & Data Wrangling in R

- Introduction to (generalized linear models)

- Bayes factors (a.k.a Bayesian p-values)

- More complex models

]



---

# Overview

- A lot of theory, but also practical sessions

- First day: Understanding Bayesian Inference

- Second day: Doing stuff in R!

- Please ask questions


---
.pull-left[

### Monday

```{r, echo = F}
tribble(
  ~Time, ~Topic,
  "[9:00 - 9:30:](#s1)" , "Welcome", 
  "[9:30 - 10:30:](#s2)" , "An Introductory Example",  
  "[10:30 - 11:30.](#s3)" , "Basics of Bayesian Statistics",	
  "[11:30 -  13:00.](#s4)", "Understanding Priors",
  "13:00 - 14:00:" , "Lunch",	
  "[14:00 - 14:45:](#s5)" , "NHST vs Bayesian Inference",	
  "[14:45 - 15:30:](#s6)" , "Exercise I", 
  "15:30 - 16:00:" , "Coffee",
  "[16:00 - 17:00:](#s7)" , "A Bit of R and Intro to 'brms'"
) %>% kable(format = "markdown")
```

]

.pull-right[

### Tuesday

```{r, echo = F}
tribble(
  ~Time, ~Topic,
  "[9:00 - 9:30:](#s1)" , "Short Recap", 
  "[9:30 - 10:30:](#s2)" , "Defining Priors",  
  "[10:30 - 11:30.](#s3)" , "Simple and Multiple Regression",	
  "[11:30 -  13:00.](#s4)", "Exercise II",
  "13:00 - 14:00:" , "Lunch",	
  "[14:00 - 14:45:](#s5)" , "Multilevel Regression",	
  "[14:45 - 15:30:](#s6)" , "Exercise III", 
  "15:30 - 16:00:" , "Coffee",
  "[16:00 - 17:00:](#s7)" , "Q&A'"
) %>% kable(format = "markdown")
```


]

---

class: inverse, center, middle

# An Introductory Example

"The Geometry of Changing Beliefs"

---

# Example: Covid-19 Rapid Test



.pull-left[

Philipp is generally **a conscientious and orderly** person. Today, he has taken a Corona self-test as he regularly does. To his surprise, the **self-test is positive**. He remembers vaguely that such tests have **a sensitivity of 95%**, i.e. they are very good at picking up the disease. Thinking about it, he actually has been **coughing** a bit lately. He immediately isolates himself and wonders:

<br>

What is the probability of the rapid test result being correct?

]

.pull-right[

![](img/boy.png)

]




---

# Intuition?

![](material/ppt_templates/Slide01.png)


---

# Reality

- We fail to acknowledge the true prevalence of the disease in the population. 

- Let's imagine for now that it is 4.8% in the population (an infection rate of 4800; currently it is rather 300, thus 0.3%)




![](material/ppt_templates/Slide02.png)
---

# New knowledge = observing evidence

- We have observed a positive test result 

- We know that the sensitivity and of such test is 95%: In 95 out of 100 cases, the test correctly identifies an infected person as positive.

![](material/ppt_templates/Slide03.png)

---

# What else?

- Such test also have a certain specificity (i.e., ability to correctly identify people who DON'T have the disease), which is also 95%. 

- This means, they produce false positives in 5% of the cases.

![](material/ppt_templates/Slide04.png)
---

# What is the true probability?

![](material/ppt_templates/Slide05.png)
---

# Non-Bayesian Thinking...

![](material/ppt_templates/Slide06.png)
---

# Bayesian Thinking: Updating beliefs

![](material/ppt_templates/Slide07.png)

---

# When should we use the Bayes Theorem?

![](material/ppt_templates/Slide08.png)

---

# How does this translate into the formula?

![](material/ppt_templates/Slide09.png)
---
# How does this translate into the formula?

![](material/ppt_templates/Slide10.png)
---
# How does this translate into the formula?

![](material/ppt_templates/Slide11.png)
---
# Multiplying probabilities

![](material/ppt_templates/Slide12.png)

---

# With actual numbers

![](material/ppt_templates/Slide13.png)
---

# The Bayes Theorem

![](material/ppt_templates/Slide14.png)
---

# Practical example: Naive Bayes classifier

.pull-left[

- Computes the prior probability ( _P_ ) for every category ( _c_ = outcome variable ) based on the training data set

- Computes the probability of every feature ( _x_ ) to be a characteristic of the class ( _c_ ); i.e., the relative frequency of the feature in category

- For every probability of a category in light of certain features ( _P(c|X)_ ), all feature probabilities ( _x_ ) are multiplied

- The algorithm hence chooses the class that has highest weighted sum of inputs

]

.pull-right[

![](https://s3.ap-south-1.amazonaws.com/techleer/204.png)


]


---

class: inverse, center, middle

# Basics of Bayesian Statistics

---

# Conceptual Framework of Bayesian Data Analysis

<br><br>

> Bayesian data analysis has two foundational ideas. The first idea is that Bayesian inference is *reallocation of credibility* across possibilities. The second foundational idea is that the possibilities, over which we allocate credibility, are *parameter values in meaningful mathematical models*. (Kruschke, 2015, p. 15)

---






