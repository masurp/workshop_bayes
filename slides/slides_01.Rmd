---
title: "Introduction to Bayesian Statistics"
subtitle: ""
author: "Philipp K. Masur"
institute: ""
date: "Mainz | 10.-11. October 2022"
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: false
      countIncrementalSlides: false
      ratio: 16:9
      titleSlideClass: [left, middle]
---

```{r child = 'theme.rmd'}
```


class: inverse, center, middle

# Welcome

Why are you interested in Bayesian Statistics?

Any prior experience?


---

# Expectations and overview

.pull-left[

### What we will cover

- Bayes Theorem and Bayesian Inference

- How to run (generalized) linear models using 'brms'

- How to specify priors and interpret results

- How to draw probabilistic inferences from results

]

.pull-left[

### What we will NOT cover

- Introduction to R & Data Wrangling in R

- Introduction to (generalized linear models)

- Bayes factors (a.k.a Bayesian p-values)

- More complex models

]



---

# Overview

- A lot of theory, but also practical sessions

- First day: Understanding Bayesian Inference

- Second day: Doing stuff in R!

- Please ask questions


---
.pull-left[

### Monday

```{r, echo = F}
tribble(
  ~Time, ~Topic,
  "[9:00 - 9:30:](#s1)" , "Welcome", 
  "[9:30 - 10:30:](#s2)" , "An Introductory Example",  
  "[10:30 - 11:30.](#s3)" , "Basics of Bayesian Statistics",	
  "[11:30 -  13:00.](#s4)", "Understanding Priors",
  "13:00 - 14:00:" , "Lunch",	
  "[14:00 - 14:45:](#s5)" , "NHST vs Bayesian Inference",	
  "[14:45 - 15:30:](#s6)" , "Exercise I", 
  "15:30 - 16:00:" , "Coffee",
  "[16:00 - 17:00:](#s7)" , "A Bit of R and Intro to 'brms'"
) %>% kable(format = "markdown")
```

]

.pull-right[

### Tuesday

```{r, echo = F}
tribble(
  ~Time, ~Topic,
  "[9:00 - 9:30:](#s1)" , "Short Recap", 
  "[9:30 - 10:30:](#s2)" , "Defining Priors",  
  "[10:30 - 11:30.](#s3)" , "Simple and Multiple Regression",	
  "[11:30 -  13:00.](#s4)", "Exercise II",
  "13:00 - 14:00:" , "Lunch",	
  "[14:00 - 14:45:](#s5)" , "Multilevel Regression",	
  "[14:45 - 15:30:](#s6)" , "Exercise III", 
  "15:30 - 16:00:" , "Coffee",
  "[16:00 - 17:00:](#s7)" , "Q&A'"
) %>% kable(format = "markdown")
```


]

---

# Literature

.pull-left[

- My slides are based on this book by Kruschke...

- ... and its translation to "brms" by Salomon Kurz ([see here](https://bookdown.org/content/3686/))

- Great and comparatively simple introduction

- Lots of examples (including R code)

]

.pull-right[
![](https://m.media-amazon.com/images/I/41je2iREauL._AC_SY580_.jpg)
]


---

class: inverse, center, middle

# An Introductory Example

"The Geometry of Changing Beliefs"

---

# Example: Covid-19 Rapid Test

.pull-left[

Philipp is generally **a conscientious and orderly** person. Today, he has taken a Corona self-test as he regularly does. To his surprise, the **self-test is positive**. He remembers vaguely that such tests have **a sensitivity of 95%**, i.e. they are very good at picking up the disease. Thinking about it, he actually has been **coughing** a bit lately. He immediately isolates himself and wonders:

<br>

What is the probability of the rapid test result being correct?

]

.pull-right[

![](img/boy.png)

]




---

# Intuition?

![](material/ppt_templates/Slide01.png)


---

# Reality

- We fail to acknowledge the true prevalence of the disease in the population. 

- Let's imagine for now that it is 4.8% in the population (an infection rate of 4800; currently it is rather 300, thus 0.3%)




![](material/ppt_templates/Slide02.png)
---

# New knowledge = observing evidence

- We have observed a positive test result 

- We know that the sensitivity and of such test is 95%: In 95 out of 100 cases, the test correctly identifies an infected person as positive.

![](material/ppt_templates/Slide03.png)

---

# What else?

- Such test also have a certain specificity (i.e., ability to correctly identify people who DON'T have the disease), which is also 95%. 

- This means, they produce false positives in 5% of the cases.

![](material/ppt_templates/Slide04.png)
---

# What is the true probability?

![](material/ppt_templates/Slide05.png)
---

# Non-Bayesian Thinking...

![](material/ppt_templates/Slide06.png)
---

# Bayesian Thinking: Updating beliefs

![](material/ppt_templates/Slide07.png)

---

# When should we use the Bayes Theorem?

![](material/ppt_templates/Slide08.png)

---

# How does this translate into the formula?

![](material/ppt_templates/Slide09.png)
---
# How does this translate into the formula?

![](material/ppt_templates/Slide10.png)
---
# How does this translate into the formula?

![](material/ppt_templates/Slide11.png)
---
# Multiplying probabilities

![](material/ppt_templates/Slide12.png)

---

# With actual numbers

![](material/ppt_templates/Slide13.png)
---

# The Bayes Theorem

![](material/ppt_templates/Slide14.png)
---

# Practical example: Naive Bayes classifier

.pull-left[

- Computes the prior probability ( _P_ ) for every category ( _c_ = outcome variable ) based on the training data set

- Computes the probability of every feature ( _x_ ) to be a characteristic of the class ( _c_ ); i.e., the relative frequency of the feature in category

- For every probability of a category in light of certain features ( _P(c|X)_ ), all feature probabilities ( _x_ ) are multiplied

- The algorithm hence chooses the class that has highest weighted sum of inputs

]

.pull-right[

![](https://s3.ap-south-1.amazonaws.com/techleer/204.png)


]

---

# A Short History of Bayesian Statistics

.pull-left[

- Thomas Bayes (1702-1761) was an English mathematician and presbyterian priest who came up with the idea of "inverse probability"

- "An essay towards solving a problem in the doctrine of chances" includes a special cases of the Bayes theorem and was published posthum in 1764 by his friend Richard Price

- Pierre-Simon Laplace (1749-1827) further develops the idea of "inverse probability"

- 1980s: Increase in use of Bayesian statistics due to Markov Chain Monte Carlo methods (computational feasability)

]
 

.pull-right[

![](https://upload.wikimedia.org/wikipedia/commons/d/d4/Thomas_Bayes.gif)

]

---

class: inverse, center, middle

# Basics of Bayesian Statistics

---

class: center, middle

# What is probability?


---

# Probability

- A long-run relative frequency
    - according to frequentist statistics (Fisher, Person, Neyman...)

<br><br>

--

- A subjective belief
    - according to Bayesian statistics (Bayes, Laplace...)

---

# Frequentist statistics explained

.pull-left[
- The "classic" statistical framework

- Is based on the idea of repeated sampling from a population

- Population parameters are fixed, actually exist

- Probability refers to the long-run frequency of a given event

- Data are random, result from sampling a fixed population distribution

]

.pull-right[

```{r, echo = F}
set.seed(42)
pop <- round(rbeta(30000,2,8)*1400+400) 
STP <- sample(pop, 50)
STP.mean <- mean(STP)
STP.sd <- sd(STP)
ggplot(NULL, aes(x = pop)) + geom_histogram(color = "white", fill = "lightblue") +
  geom_vline(xintercept = mean(pop), color = "black", linetype = "dashed") +
  theme_classic() +
  labs(x = "Income in EUR",
       title = "Income distribution of Uni Mainz students")
```

]
---
# Sampling from the population

```{r, echo = F, fig.height=6, fig.width = 15}
sample1000 <- replicate(1000, (sample(pop, 50)))
sample1000.mean <- apply(sample1000, 2, mean)
sample1000.sd <- apply(sample1000, 2, sd)
data.frame(sample1000[, 1:20]) %>%
  gather(key, value) %>%
  mutate(facet =  rep(1:20, each = 50),
         mean = rep(sample1000.mean[1:20], each = 50)) %>%
  as_tibble %>%
  ggplot() + 
  geom_histogram(aes(value), fill = "lightblue", color = "white", 
                 binwidth=50) + 
  geom_vline(aes(xintercept = mean), 
             color = "red") + 
  labs(title = "Income of the first 20 samples (n = 50)", 
       x = "Income in EUR", 
       y = "count") + 
  xlim(400, 1600) + 
  facet_wrap(~ facet) +
  theme_bw() +
  theme(panel.grid = element_blank())
```


---

# Parameter distribution

.pull-left[

```{r, echo = F}
ggplot(data.frame(sample1000.mean)) + 
  geom_histogram(aes(sample1000.mean), fill = "lightblue", color = "white", 
                 binwidth=10) + 
  geom_vline(xintercept = mean(pop), 
             color = "red") + 
  labs(title = "Means of 1000 samples with n = 50", 
       y = "Count", 
       x = paste0('Income (M(population) = ',round(mean(pop), 0),', SD = ',round(sd(sample1000.mean),1))) +
  theme_classic()
```

]

.pull-right[


- According to the central limit theorem, the means are distributed around the true population parameter *M* = `r round(mean(pop))`. 

- The standard error (*SE* = $SD(x)/\sqrt(n-1)$), which we can computed based on a single sample, is an estimate for the variance of this mean distribution

- In our first sample, the standard error is *SE* = $164/\sqrt(50-1)$ = `r round(STP.sd/sqrt(49),1)`
]

---
# Conceptual Framework of Bayesian Data Analysis

<br><br>

> Bayesian data analysis has two foundational ideas. The first idea is that Bayesian inference is *reallocation of credibility* across possibilities. The second foundational idea is that the possibilities, over which we allocate credibility, are *parameter values in meaningful mathematical models*. (Kruschke, 2015, p. 15)

---

# Reallocation of credibility across possibilites

.pull-left[

```{r, echo = F}
tibble(mu = 1:4,
       p  = .25) %>% 
  expand(nesting(mu, p), 
         x = seq(from = -2, to = 6, by = .1)) %>% 
  mutate(density = dnorm(x, mean = mu, sd = 1.2)) %>% 
  mutate(d_max = max(density)) %>% 
  mutate(rescale = p / d_max) %>% 
  mutate(density = density * rescale) %>% 
  
  # plot!
  ggplot(aes(x = x)) +
  geom_col(data = . %>% distinct(mu, p),
           aes(x = mu, y = p),
           fill = "grey67", width = 1/3) +
  geom_line(aes(y = density, group = mu)) +
  scale_x_continuous(breaks = 1:4) +
  scale_y_continuous(breaks = 0:5 / 5) +
  coord_cartesian(xlim = c(0, 5),
                  ylim = c(0, 1)) +
  labs(title = "Prior",
       x = "Possibilities", 
       y = "Credibility") +
  theme_classic() +
  theme(axis.ticks.x = element_blank(),
        panel.grid = element_blank())
```
]

--

.pull-right[

```{r, echo = F}
# Posterior
tibble(mu = 1:4,
       p  = c(.11, .56, .31, .02)) %>% 
  expand(nesting(mu, p), 
         x = seq(from = -2, to = 6, by = .1)) %>% 
  mutate(density = dnorm(x, mean = mu, sd = 1.2)) %>% 
  mutate(d_max = max(density)) %>% 
  mutate(rescale = p / d_max) %>% 
  mutate(density = density * rescale) %>% 
  
  # plot!
  ggplot() +
  geom_col(data = . %>% distinct(mu, p),
           aes(x = mu, y = p),
           fill = "grey67", width = 1/3) +
  geom_line(aes(x = x, y = density, group = mu)) +
  geom_point(data = tibble(x = c(1.75, 2.25, 2.75), y = 0),
             aes(x = x, y = y),
             size = 3, color = "grey33", alpha = 3/4) +
  scale_x_continuous(breaks = 1:4) +
  scale_y_continuous(breaks = 0:5 / 5) +
  coord_cartesian(xlim = c(0, 5),
                  ylim = c(0, 1)) +
  labs(title = "Posterior",
       x = "Possibilities", 
       y = "Credibility") +
  theme_classic() +
  theme(axis.ticks.x = element_blank(),
        panel.grid = element_blank())
```

]

---

# Logic of Bayesian Inference


- the essence of Bayesian inference is reallocation of credibility across possibilities

- The distribution of credibility initially reflects prior knowledge about the possibilities (which can be quite vague). 

- Then new data are observed, and the credibility is re-allocated. 

- Possibilities that are consistent with the data garner more credibility, while possibilities that are not consistent with the data lose credibility. 

- Bayesian analysis is the mathematics of re-allocating credibility in a logically coherent and precise way. 

_(Kruschke, 2015, p. 22)_

---


# Steps of Bayesian Data Analysis

1. Identify the **data relevant to the research questions**. What are the measurement scales of the data? Which data variables are to be predicted, and which data variables are supposed to act as predictors?

2. Define a **descriptive model** for the relevant data. The mathematical form and its parameters should be meaningful and appropriate to the theoretical purposes of the analysis.

3. Specify a **prior distribution** on the parameters. The prior must pass muster with the audience of the analysis, such as skeptical scientists.

4. Use Bayesian inference to **re-allocate credibility** across parameter values. Interpret the posterior distribution with respect to theoretically meaningful issues (assuming that the model is a reasonable description of the data; see next step).

5. Check that the **posterior predictions** mimic the data with reasonable accuracy (i.e., conduct a “posterior predictive check”). If not, then consider a different descriptive model.

---

# An example: Social media use and well-being

```{r, echo = F}
set.seed(42)
n <- 50
use <- rnorm(n, 0, 1)
wb <- -.20*use + rnorm(n, 0, .5)
wb <- scales::rescale(wb, to = c(1, 5)) %>% round(2)
d <- tibble(use, wb)
d
```

---

# Frequentist linear regression

.pull-left[

```{r, echo = F}
ggplot(d, aes(x = use, y = wb)) +
  geom_point(shape = 1) +
  geom_smooth(method = "lm", color = "black") +
  theme_classic() +
  labs( x = "Social Media Use",
       y = "Well-Being")
```

]

.pull-left[

- We can define this model with the following formula:

<center>$y_i = \beta_0 + \beta_1x + \epsilon$</center>

- or alternatively as:

<center>$y_i = \beta_0 + \beta_1x$</center>

<center>$y \sim normal(y_i, \sigma)$</center>

]


---

# Bayesian linear regression

.pull-left[

```{r, echo = F}
library(brms)
#fit <- 
#  brm(data = d, 
#      wb ~ 1 + use,
#      prior = c(prior(normal(0, 10), class = Intercept),
#                prior(normal(0, 10), class = b),
#                prior(cauchy(0, 10),  class = sigma)),
#      chains = 2, cores = 2, iter = 1000, warmup = 500,
#      seed = 2)
# save(fit, file = "slides/material/results/fit.RData")
load("material/results/fit.RData")

draws <- as_draws_df(fit)

# this will subset the output
n_lines <- 150

# plot!
draws %>% 
  slice(1:n_lines) %>% 

  ggplot() +
  geom_abline(aes(intercept = b_Intercept, slope = b_use, group = .draw),
              color = "grey50", size = 1/4, alpha = .3) +
  geom_point(data = d,
             aes(x = use, y = wb),
             shape = 1) +
  labs(title = "Multiple credible regression lines",
       x = "Social Media Use",
       y = "Well-Being") +
  theme_classic()

```

]

.pull-right[

```{r, echo = F}
library(tidybayes)

draws %>% 
  ggplot(aes(x = b_use, y = 0)) +
  stat_histinterval(point_interval = mode_hdi, .width = .95,
                    fill = "grey67", slab_color = "grey92",
                    breaks = 40, slab_size = .2, outline_bars = T) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(title = "The posterior distribution",
       subtitle = "The mode and 95% HPD intervals are\nthe dot and horizontal line at the bottom.",
       x = expression(beta[1]~(slope))) +
  theme_classic()
```

]

---

