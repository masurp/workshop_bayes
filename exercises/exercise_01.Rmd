---
title: 'Intro to Bayesian Statistic: Basics and distributions'
author: "Philipp Masur"
date: "2022-09"
output: 
  github_document:
    toc: yes
editor_options: 
  chunk_output_type: console
---

```{r, echo=F, message=F}
knitr::opts_chunk$set(echo = TRUE, results = TRUE, message = FALSE, warning = FALSE)
library(printr)
library(tidyverse)
```


In this tutorial, we are going to explore some basics of Bayesian statistics, including exercises to understand what it means to sample from a distribution, mathematically deriving posterior distributions, etc. If you are new to Bayesian statistics, consider working through [these slides]() which introduce the basic concepts and exemplify Bayesian Analysis based on real-world data.


# Basics of data simulation

## Creating vectors of numbers

```{r}
# A vector of 6 values
c(1, 1, 1, 0, 0, 0)
c(rep(1, 3), rep(0, 3))

# Consecutive numbers
c(1:10)
```



## Sampling from a vector

```{r}
# Random numbers from a vector
sample(c(1:100), 2)
```


## Replicating procedures

```{r}
replicate(n = 2, expr = sample(c(1:100), 2))
replicate(n = 2, expr = sample(c(1:100), 2), simplify = F)
replicate(n = 2, expr = sample(c(1:100), 2)) %>% as.vector
```


# Sampling from distributions

## Random generation of numbers drawn from a distribution

```{r}
# Uniform distribution
runif(n = 1, min = 0, max = 10)

# Normal (gaussian) distribution
rnorm(n = 1, mean = 0, sd = 1)
rnorm(10, 5, 2)

# Beta distribution
rbeta(10, 1, 1)

# Binomial distribution
rbinom(n = 1, size = 10, .5)
replicate(10, rbinom(n = 1, size = 10, .5))
```

**Question:** Produce a beta distribution with n = 1000 that corresponds to a normal distribution. How would you plot this distribution? (Don't forget to load packages that you might need)

```{r}
x <- rbeta(1000, 30, 30)

ggplot(NULL, aes(x = x)) +
  geom_histogram(color = "white", fill = "steelblue") +
  theme_classic()
```


## Density functions

Remember the coin toss? How can we estimate the 

```{r}
dbinom(x = 4, size = 10, prob = .5)
```


```{r}
x <- seq(0, 1, by = .001)
y <- dbinom(4, 10, x)
ggplot(NULL, aes(x = x, y = y)) +
  geom_line() +
  theme_classic()
```


**Question:** How could we produce a beta density plot with the shapes $\alpha$ = 2 and $\beta$ = 20 across probabilities from 0 to 1? How would you plot this?

```{r}
x <- seq(0, 1, by = .001)
y <- dbeta(x, 2, 20)
ggplot(NULL, aes(x = x, y = y)) +
  geom_line() +
  theme_classic()
```

# The coin toss example

## Prior belief

```{r}
d <- tibble(prob = seq(0, 1, by = .001),     
            prior = dbeta(prob, 30, 30)) %>%  
  mutate(prior = prior/sum(prior))   # standardization

# Plot
d %>%
  ggplot(aes(x = prob, 
             y = prior)) +
  geom_line(color = "red") +
  theme_minimal() +
  labs(x = expr(theta), 
       y = "", 
       title = "Prior",
       subtitle = "Beta(30, 30)")
```


## Likelihood

```{r}
# Coin toss
set.seed(5)
toss <- sample(c("tails", "heads"), size = 10, replace = T)

# Summary
table(toss)

# Frequency Plot
ggplot(NULL, 
       aes(x = toss)) + 
  geom_bar(fill = "steelblue", width = .5) + 
  scale_y_continuous(n.breaks = 5, limits = c(0, 8)) +
  theme_minimal() +
  labs(x = "")

# Compute likelihood
d <- d %>%
  mutate(likelihood = dbinom(x = 2, size = 10, prob = prob), 
         likelihood = likelihood/sum(likelihood)) 

# Plot
d %>%
  gather(key, value, -prob) %>%
  ggplot(aes(x = prob, y = value, color = key)) +
  geom_line() +
  theme_minimal() +
  labs(x = expr(theta), 
       y = "", 
       color = "",
       title = "Prior & Likelihood")
```

## Posterior

```{r}
d <- d %>%
  mutate(posterior = (prior*likelihood)/sum(prior*likelihood))

d %>%
  gather(key, value, -prob) %>%
  mutate(factor(key, 
         levels = c("prior", "likelihood", "posterior"))) %>%
  ggplot(aes(x = prob, y = value, color = key)) +
  geom_line() +
  scale_color_manual(values = c("darkblue", "black", "red")) +
  theme_minimal() +
  labs(x = expr(theta), y = "", color = "",
       title = "Prior, likelihood & posterior")
```



**Question:** Let's practice. Let's say we roll the dice again. Can we use our previous posterior as new prior and our new data as new likelihood? How does the posterior look like? What happens if we toss the coin 100 times?

Remember, we can computer the posterior beta distribution like so:

$beta(30, 30) * Binomial(2, 10, \theta)$

$beta(2 + 30, 10-2 + 30)$

```{r}
set.seed(12)
toss2 <- sample(c("tails", "heads"), size = 100, replace = T)

# Summary
table(toss2)

d2 <- tibble(prob = seq(0, 1, by = .001),
             prior = dbeta(prob, 32, 38),
             likelihood = dbinom(58, 100, prob)) %>%
  mutate(prior = prior/sum(prior),
         likelihood = likelihood/sum(likelihood),
         posterior = prior*likelihood/(sum(prior*likelihood)))

d2 %>%
  gather(key, value, -prob) %>%
  mutate(factor(key, 
         levels = c("prior", "likelihood", "posterior"))) %>%
  ggplot(aes(x = prob, y = value, color = key)) +
  geom_line() +
  scale_color_manual(values = c("darkblue", "black", "red")) +
  theme_minimal() +
  labs(x = expr(theta), y = "", color = "",
       title = "Prior, likelihood & posterior")
```

